{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1sZLWBh-U5HBa3TenmsszMDsXxJvLxEng",
      "authorship_tag": "ABX9TyPoGcBqbwJRhs/1w5T2iX60",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanmzeidler/CS710-Homework2/blob/main/Zeidler_Programming_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs, downloads, and imports"
      ],
      "metadata": {
        "id": "q1PeB9VMUj4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qQ3idEXQXdo",
        "outputId": "d8322323-c8a4-4e65-c622-4489887923ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "44ttlXJaTpFw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "Hw00rjzyVMqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Provided by Professor He\n",
        "import nltk\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "import sys\n",
        "\n",
        "def read_txt_files_from_directory(directory_path):\n",
        "    file_contents = {}\n",
        "    try:\n",
        "        for filename in os.listdir(directory_path):\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(directory_path, filename)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        file_contents[filename] = file.read()\n",
        "                except Exception as e:\n",
        "                    print(f\"An error occurred while reading {filename}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while accessing the directory: {e}\")\n",
        "        return {}\n",
        "    return file_contents\n",
        "\n",
        "def load_from_json(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as json_file:\n",
        "            data = json.load(json_file)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the file: {e}\")\n",
        "        return None\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_sentence = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            filtered_sentence.append(token)\n",
        "    return filtered_sentence\n",
        "\n",
        "def stemming(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def pre_process(text):\n",
        "    text_lower = text.lower()\n",
        "    tokens_no_punctuation = remove_punctuation(text_lower)\n",
        "    filtered_tokens = remove_stop_words(tokens_no_punctuation)\n",
        "    stemmed_tokens = stemming(filtered_tokens)\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "id": "LWp86ThuVSbe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "ejD8_ufEwCzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def tfidf_similarity(data):\n",
        "  tfidf = TfidfVectorizer()\n",
        "  vector_matrix = tfidf.fit_transform(data['patient'])\n",
        "  sim_matrix = cosine_similarity(vector_matrix)\n",
        "  top5 = np.argpartition(-sim_matrix, range(6), axis=1)[:, 1:6]\n",
        "  data['top5_tfidf'] = top5.tolist()\n",
        "  data['top5_tfidf'] = data['top5_tfidf'].apply(lambda x: id_list(data,x))\n",
        "  print(data.head())\n",
        "\n",
        "\n",
        "def id_list(data,index_list):\n",
        "  id_list = []\n",
        "  for i in index_list:\n",
        "    id_list.append(data.at[i,'patient_uid'])\n",
        "  return id_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwuQ3gU-wGIp",
        "outputId": "3182c2d3-4669-4dba-f627-1be4da9be0f2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   patient_id patient_uid      PMID                         file_path  \\\n",
            "0           0   7665777-1  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
            "1           1   7665777-2  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
            "2           2   7665777-3  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
            "3           3   7665777-4  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
            "4           4   7665777-5  33492400  comm/PMC007xxxxxx/PMC7665777.xml   \n",
            "\n",
            "                                               title  \\\n",
            "0  Early Physical Therapist Interventions for Pat...   \n",
            "1  Early Physical Therapist Interventions for Pat...   \n",
            "2  Early Physical Therapist Interventions for Pat...   \n",
            "3  Early Physical Therapist Interventions for Pat...   \n",
            "4  Early Physical Therapist Interventions for Pat...   \n",
            "\n",
            "                                             patient               age gender  \\\n",
            "0  This 60-year-old male was hospitalized due to ...  [[60.0, 'year']]      M   \n",
            "1  A 39-year-old man was hospitalized due to an i...  [[39.0, 'year']]      M   \n",
            "2  One week after a positive COVID-19 result this...  [[57.0, 'year']]      M   \n",
            "3  This 69-year-old male was admitted to the ICU ...  [[69.0, 'year']]      M   \n",
            "4  This 57-year-old male was admitted to the ICU ...  [[57.0, 'year']]      M   \n",
            "\n",
            "                                   relevant_articles  \\\n",
            "0  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
            "1  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
            "2  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
            "3  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
            "4  {'32320506': 1, '32293716': 1, '23219649': 1, ...   \n",
            "\n",
            "                                    similar_patients  \\\n",
            "0  {'7665777-2': 2, '7665777-3': 2, '7665777-4': ...   \n",
            "1  {'7665777-1': 2, '7665777-3': 2, '7665777-4': ...   \n",
            "2  {'7665777-1': 2, '7665777-2': 2, '7665777-4': ...   \n",
            "3  {'7665777-1': 2, '7665777-2': 2, '7665777-3': ...   \n",
            "4  {'7665777-1': 2, '7665777-2': 2, '7665777-3': ...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [60, year, old, male, hospit, due, moder, ard,...   \n",
            "1  [39, year, old, man, hospit, due, increasingli...   \n",
            "2  [one, week, posit, covid, 19, result, 57, year...   \n",
            "3  [69, year, old, male, admit, icu, dri, cough, ...   \n",
            "4  [57, year, old, male, admit, icu, dyspnea, hea...   \n",
            "\n",
            "                                     doc2vec_vectors  \\\n",
            "0  [0.8943244, -0.8590357, -0.8483025, 1.2250333,...   \n",
            "1  [1.6011344, -0.7558962, 2.1948078, 0.39775735,...   \n",
            "2  [1.8912835, -0.5788574, 0.47349006, 0.06720681...   \n",
            "3  [2.2874966, 0.16139041, 0.19567752, 0.9972271,...   \n",
            "4  [-0.2501597, -0.6312111, 2.3345742, -0.3232804...   \n",
            "\n",
            "                                       tfidf_vectors  \\\n",
            "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
            "\n",
            "                                          top5_tfidf  \n",
            "0  [7665777-3, 7665777-2, 7665777-7, 6011174-1, 8...  \n",
            "1  [7665777-1, 7665777-3, 6004679-1, 6011170-1, 6...  \n",
            "2  [7665777-1, 7665777-2, 7665777-7, 6011174-1, 8...  \n",
            "3  [7665777-3, 7665777-9, 8698451-1, 7665777-7, 7...  \n",
            "4  [7665777-11, 7665777-9, 7665777-10, 7665777-3,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc2Vec Vectorizer"
      ],
      "metadata": {
        "id": "D1fQVKosv-xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "def load_dataset(fname,nrows=1000):\n",
        "  data = safe_read_csv(fname,nrows)\n",
        "  data['tokens'] = data['patient'].apply(lambda text: pre_process(text))\n",
        "  print(\"Data loaded\")\n",
        "  return data\n",
        "\n",
        "def safe_read_csv(fname,nrows):\n",
        "  try:\n",
        "    filepath = os.path.join(PROJ_DIRECTORY,fname)\n",
        "    return pd.read_csv(filepath_or_buffer=filepath,nrows=nrows)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    return None\n",
        "\n",
        "#Adapted from https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py\n",
        "def read_corpus(data, tokens_only = False):\n",
        "    documents = data['tokens']\n",
        "    for i, tokens in documents.items():\n",
        "        if tokens_only:\n",
        "            yield tokens\n",
        "        else:\n",
        "            # For training data, add tags\n",
        "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "def train_doc2vec(data):\n",
        "  model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
        "  train_corpus = list(read_corpus(data))\n",
        "  test_corpus = list(read_corpus(data, tokens_only=True))\n",
        "  model.build_vocab(train_corpus)\n",
        "  print(\"Vocab built\")\n",
        "  print(\"Starting training\")\n",
        "  model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "  fname = get_tmpfile(os.path.join(PROJ_DIRECTORY,\"my_doc2vec_model\"))\n",
        "  model.save(fname)\n",
        "  print(\"Model saved\")\n",
        "  return model\n",
        "\n",
        "def calc_doc2vec_vectors(model, data):\n",
        "  print(\"Calcualting Doc2Vec vectors\")\n",
        "  data['doc2vec_vectors'] = data['tokens'].apply(lambda tokens: model.infer_vector(tokens))\n",
        "  print(\"Vectors calculated\")\n",
        "\n",
        "\n",
        "def load_model(fname):\n",
        "  return Doc2Vec.load(fname)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OBmUsM-WgAn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJ_DIRECTORY =\"/content/drive/MyDrive/HI744_Programming_Assignment_1\"\n",
        "data = load_dataset(\"PMC-Patients.csv\")\n",
        "doc2vec_model = train_doc2vec(data)\n",
        "calc_doc2vec_vectors(doc2vec_model,data)\n",
        "calc_tfidf_vectors(data)\n",
        "tfidf_similarity(data)\n",
        "# PROJ_DIRECTORY = os.getcwd()\n",
        "# print(data.head())\n",
        "# train_corpus = list(read_corpus(data))\n",
        "# test_corpus = list(read_corpus(data, tokens_only=True))\n",
        "# print(train_corpus[:3])\n",
        "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
        "# model.build_vocab(train_corpus)\n",
        "# model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "# print(data['tokens'][1])\n",
        "# vector = model.infer_vector(data['tokens'][1])\n",
        "# print(vector)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YVOmNrXX1bq",
        "outputId": "0dda59d9-9e5b-48a1-e8b3-c0aac8b07fd9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 233817 stored elements and shape (1000, 22682)>\n",
            "  Coords\tValues\n",
            "  (0, 20766)\t0.11143435450245165\n",
            "  (0, 1018)\t0.04561520420845719\n",
            "  (0, 22583)\t0.016681030483014603\n",
            "  (0, 14971)\t0.016107544895133923\n",
            "  (0, 12350)\t0.035949987098507566\n",
            "  (0, 22296)\t0.04827438002199669\n",
            "  (0, 9825)\t0.06286041449671415\n",
            "  (0, 6840)\t0.03114373070050666\n",
            "  (0, 20957)\t0.21383889413556914\n",
            "  (0, 13100)\t0.049056769114834364\n",
            "  (0, 2539)\t0.08488007200799244\n",
            "  (0, 8626)\t0.04642679869460891\n",
            "  (0, 5469)\t0.061125912015148276\n",
            "  (0, 350)\t0.04530875434266244\n",
            "  (0, 22458)\t0.048467692563144406\n",
            "  (0, 20300)\t0.03355771061218345\n",
            "  (0, 14946)\t0.0479379587231723\n",
            "  (0, 8172)\t0.04202962781042252\n",
            "  (0, 6813)\t0.06625278820444788\n",
            "  (0, 5429)\t0.10591473886133504\n",
            "  (0, 2174)\t0.17524650765915553\n",
            "  (0, 6928)\t0.11408610498601464\n",
            "  (0, 22333)\t0.14137716502517184\n",
            "  (0, 7275)\t0.08753352991225126\n",
            "  (0, 18993)\t0.04830621254473597\n",
            "  :\t:\n",
            "  (999, 17858)\t0.08411498025188792\n",
            "  (999, 4153)\t0.0849931635778148\n",
            "  (999, 7781)\t0.09778608780728298\n",
            "  (999, 2499)\t0.10472798345733425\n",
            "  (999, 18518)\t0.08904033359898283\n",
            "  (999, 21165)\t0.09022154822111018\n",
            "  (999, 19910)\t0.0868974377979775\n",
            "  (999, 6076)\t0.09598222924903409\n",
            "  (999, 17066)\t0.0943504406947086\n",
            "  (999, 16988)\t0.26069231339393245\n",
            "  (999, 3259)\t0.0914903375675681\n",
            "  (999, 15168)\t0.0859188388101368\n",
            "  (999, 20379)\t0.10784947824618027\n",
            "  (999, 15645)\t0.19557217561456597\n",
            "  (999, 2743)\t0.11659523245448043\n",
            "  (999, 11792)\t0.10784947824618027\n",
            "  (999, 19037)\t0.1116698791073855\n",
            "  (999, 21202)\t0.11659523245448043\n",
            "  (999, 21350)\t0.2094559669146685\n",
            "  (999, 2608)\t0.10784947824618027\n",
            "  (999, 21823)\t0.12353712810453168\n",
            "  (999, 13365)\t0.12353712810453168\n",
            "  (999, 21365)\t0.12353712810453168\n",
            "  (999, 12933)\t0.12353712810453168\n",
            "  (999, 2987)\t0.12353712810453168\n"
          ]
        }
      ]
    }
  ]
}